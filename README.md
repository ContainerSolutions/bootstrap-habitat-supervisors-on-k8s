# Bootstrapping Habitat supervisors on top of kubernets


[Habitat](http://habitat.sh) supervisors gossip with each other to support changing the
configuration during runtime, and to have support e.g. leader/follower topologies.

**TL;WR**: Settting up habitat supervisors on top of K8s is not trivial.
This project solves that via `Statefulset`s.
See the [Usage](#usage) section below on how to use it.

## Background
Let's say that we want to run a high available redis cluster.
Therefore, we opt to have a kubernetes deployment, which will create a replicaset.
The supervisors in the pods will perform leader election, after which we'll apply
the correct kubernetes labels to those pods, such that the pod that won the election will
be selected by the `redis-master` service, and the other two pods by the `redis-slave` service.

<img src="doc/images/ideal.png">

### The problem
Setting this up on kubernetes can be tricky, since habitat requires that the initial node
is started _without_ the `--peer` option. If you'd use a `ReplicaSet`, you can only create
identical pods, and therefore either all of them will have the `--peer` option, or none of them
will. In the first case, nothing will happen because the supervisor will try to join the peer node
before allowing other nodes to join it, and in the latter none will ever join, and therefore
they will not be linked to the `redis-master` and `redis-slave` services.

<img src="doc/images/problem.png">

### The solution
Luckily kubernetes supports `Statefullsets`, which allow pods to have stable identifier names, and
a guarranteed startup order.

The container that is generated by the `build.sh` script will run a small 
[ruby script](k8s-hab-sup-bootstrap/src/sup-bootstrap-init.rb), that
determines what the arguments to the habitat supervisor must be to sucessfully start.

The first pod in the Statefulset (with the name `hab-sup-boostrap-0`) will not have any 
`--peer` flag, the second will point to just `hab-sup-bootstrap-0`, the third to
both `hab-sup-bootstrap-0` and `hab-sup-bootstrap-1` etc.

Our redis cluster can just point to the stable `hab-bootstrap` name to bootstrap the gossiping 
(purple arrows)

<img src="doc/images/statefulset.png">

### Remaining problems
AFAICS there is one remaining source of race issues. If the `hab-sup-bootstrap-0` pod dies,
and is restarted, it might take some (smallish) amount of time before the other boostrap
supervisors notice that it's up again, and join it.

During this interval, a split-brain problem could occur.
Consider other habitat supervisors that run a leader topology, which might connect
to `hab-sup-bootstrap-0` and conclude that one of them is the leader, whilst actually there
are older pods that are just connected to `hab-sup-bootstrap-{1,2}`.

I believe this can only be fixed if the supervisors would have a flag to force connecting it to
all IP addresses that are resolved by the service IP.


# Usage
To test it out on minikube:

1. Start minikube `minikube start`
2. Set your `ORIGIN` env var: `export ORIGIN=my_origin`
3. Run the `build-bootstrap.sh` script. 
4. Import the container in minikube: 
    `docker save $ORIGIN/k8s-sup-bootstrap-init | (eval $(minikube docker-env); docker load)`
5. Apply the manifest: `kubectl apply -f bootstrap-hab-sup-on-k8s.yml`
